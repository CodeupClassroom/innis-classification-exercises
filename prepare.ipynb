{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "\n",
    "Plan - Acquire - **Prepare** - Explore - Model - Deliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we are doing and why:\n",
    "\n",
    "**What:** Clean and tidy our data so that it is ready for exploration, analysis and modeling\n",
    "\n",
    "**Why:** Set ourselves up for certainty! \n",
    "\n",
    "    1) Ensure that our observations will be sound:\n",
    "        Validity of statistical and human observations\n",
    "    2) Ensure that we will not have computational errors:\n",
    "        non numerical data cells, nulls/NaNs\n",
    "    3) Protect against overfitting:\n",
    "        Ensure that have a split data structure prior to drawing conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level Roadmap:\n",
    "\n",
    "#### **Input:** An aquired dataset (One Pandas Dataframe) ------> **Output:** Tidied and cleaned data split into Train, Â Validate, and Test sets (Three Pandas Dataframes)\n",
    "\n",
    "#### **Processes:** Summarize the data ---> Clean the data ---> Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# numpy for vectorized operations\n",
    "import numpy as np\n",
    "# pandas for dataframe manipulation of tabular data\n",
    "import pandas as pd\n",
    "# matplotlib for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train test split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# imputer from sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# our own acquire script:\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab our acquired dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeaways from df head:\n",
    "# survived is our target -- it is not a potential feature\n",
    "# \n",
    "# passenger id is probably inadvisable to keep in\n",
    "# pclass and class appear to be the same thing\n",
    "# embark_town and embarked appear to be the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns will give just the column names as strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe our object columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe object columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather our takeaways, i.e., what we are going to do when we clean:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_duplicates...run just in case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively: df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if anything was dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with too many missing to have any value right now\n",
    "# drop columns that duplicate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['deck', 'age', 'embarked', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could fill embark_town with most common value, 'Southampton', by hard-coding the value using the fillna() function, as below. Or we could use an imputer. We will demonstrate the imputer *after* the train-validate-test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows: r0ws\n",
    "# cols: co1s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to perform these steps when we need to reproduce our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_titanic_data(df):\n",
    "    '''\n",
    "    takes in a dataframe of the titanic dataset as it is acquired and returns a cleaned dataframe\n",
    "    arguments: df: a pandas DataFrame with the expected feature names and columns\n",
    "    return: clean_df: a dataframe with the cleaning operations performed on it\n",
    "    '''\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.drop(columns=['deck', 'embarked', 'class', 'age'])\n",
    "    df['embark_town'] = df.embark_town.fillna(value='Southampton')\n",
    "    dummy_df = pd.get_dummies(df[['sex', 'embark_town']], drop_first=[True,True])\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df.drop(columns=['sex', 'embark_town'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% test, 80% train_validate\n",
    "# then of the 80% train_validate: 30% validate, 70% train. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on original dataframe for clarity of the imputer example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option for Missing Values: Impute\n",
    "\n",
    "We can impute values using the mean, median, mode (most frequent), or a constant value. We will use sklearn.imputer.SimpleImputer to do this.  \n",
    "\n",
    "1. Create the imputer object, selecting the strategy used to impute (mean, median or mode (strategy = 'most_frequent'). \n",
    "2. Fit to train. This means compute the mean, median, or most_frequent (i.e. mode) for each of the columns that will be imputed. Store that value in the imputer object. \n",
    "3. Transform train: fill missing values in train dataset with that value identified\n",
    "4. Transform test: fill missing values with that value identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the `SimpleImputer` object, which we will store in the variable `imputer`. In the creation of the object, we will specify the strategy to use (`mean`, `median`, `most_frequent`). Essentially, this is creating the instructions and assigning them to a variable we will reference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent', missing_values=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `Fit` the imputer to the columns in the training df.  This means that the imputer will determine the `most_frequent` value, or other value depending on the `strategy` called, for each column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = imputer.fit(train[['embark_town']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. It will store that value in the imputer object to use upon calling `transform.` We will call `transform` on each of our samples to fill any missing values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train[['embark_town']] = imputer.transform(train[['embark_town']])\n",
    "validate[['embark_town']] = imputer.transform(validate[['embark_town']])\n",
    "test[['embark_town']] = imputer.transform(test[['embark_town']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will run through all of these steps, when I provide a train and test dataframe, a strategy, and a list of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impute_mode(train, validate, test):\n",
    "    '''\n",
    "    impute mode for embark_town\n",
    "    '''\n",
    "    imputer = SimpleImputer(strategy='most_frequent', missing_values=np.nan)\n",
    "    train[['embark_town']] = imputer.fit_transform(train[['embark_town']])\n",
    "    validate[['embark_town']] = imputer.transform(validate[['embark_town']])\n",
    "    test[['embark_town']] = imputer.transform(test[['embark_town']])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blend the clean, split and impute functions into a single prep_data() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_titanic_data(df):\n",
    "    '''\n",
    "    takes in a dataframe of the titanic dataset as it is acquired and returns a cleaned dataframe\n",
    "    arguments: df: a pandas DataFrame with the expected feature names and columns\n",
    "    return: train, test, split: three dataframes with the cleaning operations performed on them\n",
    "    '''\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.drop(columns=['deck', 'embarked', 'class', 'age', 'passenger_id'])\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=1349, stratify=df.survived)\n",
    "    train, validate = train_test_split(train, train_size=0.7, random_state=1349, stratify=train.survived)\n",
    "#     print(train.info())\n",
    "#     return train, validate, test\n",
    "    train, validate, test = impute_mode(train, validate, test)\n",
    "    dummy_train = pd.get_dummies(train[['sex', 'embark_town']], drop_first=[True,True])\n",
    "    dummy_validate = pd.get_dummies(validate[['sex', 'embark_town']], drop_first=[True,True])\n",
    "    dummy_test = pd.get_dummies(test[['sex', 'embark_town']], drop_first=[True,True])\n",
    "    train = pd.concat([train, dummy_train], axis=1)\n",
    "    validate = pd.concat([validate, dummy_validate], axis=1)\n",
    "    test = pd.concat([test, dummy_test], axis=1)\n",
    "    train = train.drop(columns=['sex', 'embark_town'])\n",
    "    validate = validate.drop(columns=['sex', 'embark_town'])\n",
    "    test = test.drop(columns=['sex', 'embark_town'])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end product of this exercise should be the specified functions in a python script named `prepare.py`.\n",
    "Do these in your `classification_exercises.ipynb` first, then transfer to the prepare.py file. \n",
    "\n",
    "This work should all be saved in your local `classification-exercises` repo. Then add, commit, and push your changes.\n",
    "\n",
    "Using the Iris Data:  \n",
    "\n",
    "1. Use the function defined in `acquire.py` to load the iris data.  \n",
    "\n",
    "1. Drop the `species_id` and `measurement_id` columns.  \n",
    "\n",
    "1. Rename the `species_name` column to just `species`.  \n",
    "\n",
    "1. Create dummy variables of the species name. \n",
    "\n",
    "1. Create a function named `prep_iris` that accepts the untransformed iris data, and returns the data with the transformations above applied.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
