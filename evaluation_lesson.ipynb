{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Evaluation\n",
    "\n",
    "## Examples:\n",
    "\n",
    "Imagine you're bringing coffee to a meeting, and you need to predict whether each person at the meeting will want a coffee or not. Which metric should you choose? It depends\n",
    "\n",
    "Outcomes:\n",
    "\n",
    "- FP: Buy a coffee for someone who won't drink it\n",
    "- FN: Don't buy a coffee for someone who wanted one\n",
    "- TP: Buy a coffee for someone who will drink it\n",
    "- TN: Don't buy a coffee for someone who wouldn't drink it anyway\n",
    "\n",
    "Scenarios\n",
    "\n",
    "- revolucion: good coffee, but expensive\n",
    "    - cost of a FP is higher than FN\n",
    "    - precision is better here because buying a cup of coffee for someone who won't drink it is expensive\n",
    "    - We want to be sure about our positive predictions\n",
    "- taco cabana: bad coffee, but cheap\n",
    "    - cost of a FN is higher than FP\n",
    "    - optimize for recall because the coffee is cheap, its not bad to buy a cheap coffee for someone who won't drink it; worse to not get someone coffee who wanted it\n",
    "- meeting with super important client\n",
    "    - cost of FN is higher, because they might be offended if we dont' get them coffee\n",
    "    - cost of FN == not signing a contract\n",
    "    - recall\n",
    "\n",
    "What if buy coffee for everyone or just don't buy any coffee? Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Exercise\n",
    "\n",
    "Scenario: Build a classifier to predict whether a given face should unlock the iPhone.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "- What are the possible outcomes?\n",
    "- What are the costs of the outcomes? cost of a FP is higher than FN\n",
    "- Which metric should we use? Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario: Predict whether an email is spam or not. Emails marked as spam skip the inbox and go to the spam folder.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "    * + = avoid spam, - = mark as span\n",
    "    * + = mark as spam, - not spam\n",
    "- What are the possible outcomes?\n",
    "- What are the costs of the outcomes? sending a real message to the spam folder is worse than a spam message getting through\n",
    "- Which metric should we use? precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario: Predict whether an email is a phishing attempt. When we predict positive, show an additional banner warning the user that this might be a phishing email.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "- What are the possible outcomes?\n",
    "- What are the costs of the outcomes?\n",
    "- Which metric should we use? recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual prediction\n",
       "0     coffee  no coffee\n",
       "1  no coffee  no coffee\n",
       "2  no coffee     coffee\n",
       "3     coffee     coffee\n",
       "4     coffee     coffee\n",
       "5     coffee     coffee\n",
       "6  no coffee  no coffee\n",
       "7     coffee  no coffee"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'actual': ['coffee', 'no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'coffee'],\n",
    "    'prediction': ['no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'no coffee'],\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>coffee</th>\n",
       "      <th>no coffee</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no coffee</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual      coffee  no coffee\n",
       "prediction                   \n",
       "coffee           3          1\n",
       "no coffee        2          2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.prediction, df.actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TP: predicted coffee + actual is coffee\n",
    "- FP: predicted coffee, but they didn't like coffee\n",
    "- FN: predicted no coffee, but really they liked coffee\n",
    "- TN: predicted no coffee, actual no coffee\n",
    "\n",
    "Note:\n",
    "\n",
    "- our choice of positive and negative is arbitrary\n",
    "- the labels / layout of the confusion matrix varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "- **accuracy**: (TP + TN) / (TP + TN + FP + FN)\n",
    "    - (3 + 2) / (3 + 1 + 2 +2) = 62.5%\n",
    "- **precision**: TP / (TP + FP)\n",
    "    - 3 / (3 + 1) = 75%\n",
    "    - FP is more costly than FN\n",
    "- **recall**: TP / (TP + FN)\n",
    "    - 3 / (3 + 2) = 60%\n",
    "    - FN is more costly than FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coffee       5\n",
       "no coffee    3\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['baseline'] = 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual prediction baseline\n",
       "0     coffee  no coffee   coffee\n",
       "1  no coffee  no coffee   coffee\n",
       "2  no coffee     coffee   coffee\n",
       "3     coffee     coffee   coffee\n",
       "4     coffee     coffee   coffee\n",
       "5     coffee     coffee   coffee\n",
       "6  no coffee  no coffee   coffee\n",
       "7     coffee  no coffee   coffee"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model accuracy\n",
    "(df.actual == df.prediction).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "(df.actual == df.baseline).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual prediction baseline\n",
      "2  no coffee     coffee   coffee\n",
      "3     coffee     coffee   coffee\n",
      "4     coffee     coffee   coffee\n",
      "5     coffee     coffee   coffee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision -- how good are our positive predictions\n",
    "# precision -- model performance | pred +\n",
    "subset = df[df.prediction == 'coffee']\n",
    "print(subset)\n",
    "(subset.prediction == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual prediction baseline\n",
      "0  coffee  no coffee   coffee\n",
      "3  coffee     coffee   coffee\n",
      "4  coffee     coffee   coffee\n",
      "5  coffee     coffee   coffee\n",
      "7  coffee  no coffee   coffee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall -- how often do we get the actual positive cases\n",
    "# recall -- model performance | actual +\n",
    "subset = df[df.actual == 'coffee']\n",
    "print(subset)\n",
    "(subset.prediction == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will the precision and recall of our baseline model that always predicts + be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual prediction baseline\n",
      "0     coffee  no coffee   coffee\n",
      "1  no coffee  no coffee   coffee\n",
      "2  no coffee     coffee   coffee\n",
      "3     coffee     coffee   coffee\n",
      "4     coffee     coffee   coffee\n",
      "5     coffee     coffee   coffee\n",
      "6  no coffee  no coffee   coffee\n",
      "7     coffee  no coffee   coffee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision\n",
    "subset = df[df.baseline == 'coffee']\n",
    "print(subset)\n",
    "(subset.baseline == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual prediction baseline\n",
      "0  coffee  no coffee   coffee\n",
      "3  coffee     coffee   coffee\n",
      "4  coffee     coffee   coffee\n",
      "5  coffee     coffee   coffee\n",
      "7  coffee  no coffee   coffee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall\n",
    "subset = df[df.actual == 'coffee']\n",
    "print(subset)\n",
    "(subset.baseline == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does \"positive\" mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model accuracy: 62.50%\n",
      "baseline accuracy: 62.50%\n",
      "\n",
      "   model recall: 66.67%\n",
      "baseline recall: 0.00%\n",
      "\n",
      "model precision: 50.00%\n",
      "baseline precision: nan%\n"
     ]
    }
   ],
   "source": [
    "positive = 'no coffee'\n",
    "\n",
    "# accuracy -- overall hit rate\n",
    "model_accuracy = (df.prediction == df.actual).mean()\n",
    "baseline_accuracy = (df.baseline == df.actual).mean()\n",
    "\n",
    "# precision -- how good are our positive predictions?\n",
    "# precision -- model performance | predicted positive\n",
    "subset = df[df.prediction == positive]\n",
    "model_precision = (subset.prediction == subset.actual).mean()\n",
    "subset = df[df.baseline == positive]\n",
    "baseline_precision = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "# recall -- how good are we at detecting actual positives?\n",
    "# recall -- model performance | actual positive\n",
    "subset = df[df.actual == positive]\n",
    "model_recall = (subset.prediction == subset.actual).mean()\n",
    "baseline_recall = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "\n",
    "print(f'   model accuracy: {model_accuracy:.2%}')\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')\n",
    "print()\n",
    "print(f'   model recall: {model_recall:.2%}')\n",
    "print(f'baseline recall: {baseline_recall:.2%}')\n",
    "print()\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In short:\n",
    "\n",
    "- accuracy doesn't tell the whole story\n",
    "- optimize for **precision** when you want to be sure about your positive predictions\n",
    "- optimize for **recall** when you don't want to miss positive cases\n",
    "- baseline model predicts the most common class (not necessarily the postive class)\n",
    "- + / - are somewhat arbitrary, but generally a postive prediction means taking action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
